

# from fastapi import FastAPI, UploadFile, File
# from pydantic import BaseModel
# from transformers import pipeline
# from huggingface_hub import hf_hub_download
# from llama_cpp import Llama
# from PIL import Image
# import io
# import hashlib
# import asyncio
# import os
# import requests
# from concurrent.futures import ThreadPoolExecutor
# from fastapi.middleware.cors import CORSMiddleware

# app = FastAPI()

# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=["*"],
#     allow_credentials=True,
#     allow_methods=["*"],
#     allow_headers=["*"],
# )

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Schemas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# class TextInput(BaseModel):
#     text: str

# class FactInput(BaseModel):
#     claim: str

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Thread pool ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CPU_CORES = os.cpu_count() or 4
# executor = ThreadPoolExecutor(max_workers=CPU_CORES)

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ In-memory cache ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# fact_cache = {}

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ API Key ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# SERPER_API_KEY = "3c6cba844457eff753d0c9cfd8cce7ffbf4b090e"

# print("–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∏—Ç–µ...")

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Image detector ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# image_detector = pipeline(
#     "image-classification",
#     model="capcheck/ai-human-generated-image-detection"
# )

# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Text detector ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# text_detector = pipeline(
#     "text-classification",
#     model="xlm-roberta-large"
# )
# #fakespot-ai/roberta-base-ai-text-detection-v1
# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ BgGPT LLM ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# print("–ò–∑—Ç–µ–≥–ª—è–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞...")
# model_path = hf_hub_download(
#     repo_id="INSAIT-Institute/BgGPT-Gemma-2-9B-IT-v1.0-GGUF",
#     filename="BgGPT-Gemma-2-9B-IT-v1.0.Q4_K_M.gguf"
# )

# print("–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∞...")
# llm = Llama(
#     model_path=model_path,
#     n_ctx=1024,
#     n_threads=CPU_CORES,
#     n_batch=512,
#     use_mlock=True,
#     verbose=False,
# )

# print("–í—Å–∏—á–∫–∏ –º–æ–¥–µ–ª–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏!")


# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Serper search with retry + fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def search_web(query: str) -> str:
#     # Try Bulgarian first, fall back to global if no results
#     for lang in (("bg", "bg"), ("us", "en")):
#         gl, hl = lang
#         for attempt in range(2):   # retry once on failure
#             try:
#                 response = requests.post(
#                     "https://google.serper.dev/search",
#                     headers={
#                         "X-API-KEY": SERPER_API_KEY,
#                         "Content-Type": "application/json"
#                     },
#                     json={
#                         "q": query,
#                         "gl": gl,
#                         "hl": hl,
#                         "num": 5,
#                         "lr": "lang_bg" if gl == "bg" else "lang_en"
#                     },
#                     timeout=6
#                 )

#                 if not response.ok:
#                     break   # bad status, try next region

#                 data = response.json()
#                 snippets = []

#                 # Answer box is the most accurate ‚Äî prioritise it
#                 if data.get("answerBox"):
#                     box = data["answerBox"]
#                     if box.get("answer"):
#                         snippets.append(box["answer"])
#                     if box.get("snippet"):
#                         snippets.append(box["snippet"])

#                 for r in data.get("organic", [])[:4]:
#                     if r.get("snippet"):
#                         snippets.append(r["snippet"])

#                 if snippets:
#                     return " | ".join(snippets)

#             except requests.Timeout:
#                 pass   # retry
#             except Exception:
#                 break  # unexpected error, skip to next region

#     return "–ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è."


# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ BgGPT inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# def run_llm(claim: str) -> str:
#     search_result = search_web(claim)
#     print(f"üìÑ –ù–∞–º–µ—Ä–µ–Ω–æ: {search_result[:200]}...")

#     context = search_result[:700]

#     prompt = f"""–ü—Ä–æ–≤–µ—Ä–∏ —Å–ª–µ–¥–Ω–æ—Ç–æ —Ç–≤—ä—Ä–¥–µ–Ω–∏–µ –∫–∞—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞—à —Å–∞–º–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞ –ø–æ-–¥–æ–ª—É.

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {context}

# –¢–≤—ä—Ä–¥–µ–Ω–∏–µ: {claim}

# –û—Ç–≥–æ–≤–æ—Ä—ä—Ç —Ç–∏ —Ç—Ä—è–±–≤–∞ –¥–∞ –∑–∞–ø–æ—á–≤–∞ –ó–ê–î–™–õ–ñ–ò–¢–ï–õ–ù–û —Å "–í—è—Ä–Ω–æ" –∏–ª–∏ "–ù–µ–≤—è—Ä–Ω–æ", –ø–æ—Å–ª–µ–¥–≤–∞–Ω–æ –æ—Ç —Ç–∏—Ä–µ –∏ –µ–¥–Ω–æ –∏–∑—Ä–µ—á–µ–Ω–∏–µ.
# –ó–∞–±—Ä–∞–Ω–µ–Ω–æ –µ –¥–∞ –ø–∏—à–µ—à "–ù–µ—è—Å–Ω–æ", "–ê–Ω–∞–ª–∏–∑" –∏–ª–∏ –∫–∞–∫–≤–æ—Ç–æ –∏ –¥–∞ –µ –¥—Ä—É–≥–æ –≤ –Ω–∞—á–∞–ª–æ—Ç–æ.
# –ü—Ä–∏–º–µ—Ä –∑–∞ –ø—Ä–∞–≤–∏–ª–µ–Ω –æ—Ç–≥–æ–≤–æ—Ä: –í—è—Ä–Ω–æ ‚Äî –ë—ä–ª–≥–∞—Ä–∏—è –µ –¥—ä—Ä–∂–∞–≤–∞ –≤ –ï–≤—Ä–æ–ø–∞.

# –û—Ç–≥–æ–≤–æ—Ä: """

#     output = llm(
#         prompt,
#         max_tokens=120,
#         temperature=0.1,
#         top_p=0.9,
#         repeat_penalty=1.1,
#         stop=["–¢–≤—ä—Ä–¥–µ–Ω–∏–µ:", "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:", "–ù–µ—è—Å–Ω–æ", "\n\n"]
#     )
#     return output["choices"][0]["text"].strip()


# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

# @app.get("/")
# def home():
#     return {"status": "AI backend —Ä–∞–±–æ—Ç–∏"}


# @app.post("/detect-image")
# async def detect_image(file: UploadFile = File(...)):
#     contents = await file.read()
#     image = Image.open(io.BytesIO(contents)).convert("RGB")
#     loop = asyncio.get_event_loop()
#     result = await loop.run_in_executor(executor, image_detector, image)
#     return {"result": result}


# @app.post("/detect-text")
# async def detect_text(data: TextInput):
#     loop = asyncio.get_event_loop()
#     result = await loop.run_in_executor(executor, text_detector, data.text)
#     return {"result": result}


# @app.post("/fact-check")
# async def fact_check(data: FactInput):
#     cache_key = hashlib.md5(data.claim.lower().strip().encode()).hexdigest()
#     if cache_key in fact_cache:
#         print("‚úÖ Cache hit")
#         return fact_cache[cache_key]

#     loop = asyncio.get_event_loop()
#     result_text = await loop.run_in_executor(executor, run_llm, data.claim)

#     response = {"result": result_text}
#     fact_cache[cache_key] = response
#     return response        




from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from transformers import pipeline
from PIL import Image
import io
import hashlib
import asyncio
import os
import requests
from concurrent.futures import ThreadPoolExecutor
from fastapi.middleware.cors import CORSMiddleware
import anthropic

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://fact-check.up.railway.app",
        "https://factcheck-noit.up.railway.app"
    ], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Schemas ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
class TextInput(BaseModel):
    text: str

class FactInput(BaseModel):
    claim: str

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Thread pool ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CPU_CORES = os.cpu_count() or 4
executor = ThreadPoolExecutor(max_workers=CPU_CORES)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ In-memory cache ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
fact_cache = {}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ API Keys ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
SERPER_API_KEY = "3c6cba844457eff753d0c9cfd8cce7ffbf4b090e"
ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)

print("–ó–∞—Ä–µ–∂–¥–∞–Ω–µ –Ω–∞ –º–æ–¥–µ–ª–∏—Ç–µ...")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Image detector ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
image_detector = pipeline(
    "image-classification",
    model="capcheck/ai-human-generated-image-detection"
)
 
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Text detector ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
text_detector = pipeline(
    "text-classification",
    model="roberta-base-openai-detector"
)

print("–í—Å–∏—á–∫–∏ –º–æ–¥–µ–ª–∏ —Å–∞ –∑–∞—Ä–µ–¥–µ–Ω–∏!")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Serper search with retry + fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def search_web(query: str) -> str:
    for lang in (("bg", "bg"), ("us", "en")):
        gl, hl = lang
        for attempt in range(2):
            try:
                response = requests.post(
                    "https://google.serper.dev/search",
                    headers={
                        "X-API-KEY": SERPER_API_KEY,
                        "Content-Type": "application/json"
                    },
                    json={
                        "q": query,
                        "gl": gl,
                        "hl": hl,
                        "num": 5,
                        "lr": "lang_bg" if gl == "bg" else "lang_en"
                    },
                    timeout=6
                )

                if not response.ok:
                    break

                data = response.json()
                snippets = []

                if data.get("answerBox"):
                    box = data["answerBox"]
                    if box.get("answer"):
                        snippets.append(box["answer"])
                    if box.get("snippet"):
                        snippets.append(box["snippet"])

                for r in data.get("organic", [])[:4]:
                    if r.get("snippet"):
                        snippets.append(r["snippet"])

                if snippets:
                    return " | ".join(snippets)

            except requests.Timeout:
                pass
            except Exception:
                break

    return "–ù—è–º–∞ –Ω–∞–º–µ—Ä–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è."


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Claude inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def run_llm(claim: str) -> str:
    search_result = search_web(claim)
    print(f"üìÑ –ù–∞–º–µ—Ä–µ–Ω–æ: {search_result[:200]}...")

    context = search_result[:700]

    prompt = f"""–û—Ç–≥–æ–≤–∞—Ä—è–π –°–ê–ú–û –Ω–∞ –±—ä–ª–≥–∞—Ä—Å–∫–∏ –µ–∑–∏–∫.

–ü—Ä–æ–≤–µ—Ä–∏ —Å–ª–µ–¥–Ω–æ—Ç–æ —Ç–≤—ä—Ä–¥–µ–Ω–∏–µ –∫–∞—Ç–æ –∏–∑–ø–æ–ª–∑–≤–∞—à —Å–∞–º–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è—Ç–∞ –ø–æ-–¥–æ–ª—É.

–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {context}

–¢–≤—ä—Ä–¥–µ–Ω–∏–µ: {claim}

–û—Ç–≥–æ–≤–æ—Ä—ä—Ç —Ç–∏ —Ç—Ä—è–±–≤–∞ –¥–∞ –∑–∞–ø–æ—á–≤–∞ –ó–ê–î–™–õ–ñ–ò–¢–ï–õ–ù–û —Å "–í—è—Ä–Ω–æ" –∏–ª–∏ "–ù–µ–≤—è—Ä–Ω–æ", –ø–æ—Å–ª–µ–¥–≤–∞–Ω–æ –æ—Ç —Ç–∏—Ä–µ –∏ –µ–¥–Ω–æ –∏–∑—Ä–µ—á–µ–Ω–∏–µ. –ó–∞–±—Ä–∞–Ω–µ–Ω–æ –µ –¥–∞ –ø–∏—à–µ—à "–ù–µ—è—Å–Ω–æ", "–ê–Ω–∞–ª–∏–∑" –∏–ª–∏ –∫–∞–∫–≤–æ—Ç–æ –∏ –¥–∞ –µ –¥—Ä—É–≥–æ –≤ –Ω–∞—á–∞–ª–æ—Ç–æ.
–ü—Ä–∏–º–µ—Ä –∑–∞ –ø—Ä–∞–≤–∏–ª–µ–Ω –æ—Ç–≥–æ–≤–æ—Ä: –í—è—Ä–Ω–æ ‚Äî –ë—ä–ª–≥–∞—Ä–∏—è –µ –¥—ä—Ä–∂–∞–≤–∞ –≤ –ï–≤—Ä–æ–ø–∞.

–û—Ç–≥–æ–≤–æ—Ä: """

    message = client.messages.create(
        model="claude-haiku-4-5-20251001",
        max_tokens=120,
        messages=[{"role": "user", "content": prompt}]
    )
    return message.content[0].text.strip()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Endpoints ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@app.get("/")
def home():
    return {"status": "AI backend —Ä–∞–±–æ—Ç–∏"}


@app.post("/detect-image")
async def detect_image(file: UploadFile = File(...)):
    contents = await file.read()
    image = Image.open(io.BytesIO(contents)).convert("RGB")
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(executor, image_detector, image)
    return {"result": result}


@app.post("/detect-text")
async def detect_text(data: TextInput):
    loop = asyncio.get_event_loop()
    result = await loop.run_in_executor(executor, text_detector, data.text)
    return {"result": result}


@app.post("/fact-check")
async def fact_check(data: FactInput):
    cache_key = hashlib.md5(data.claim.lower().strip().encode()).hexdigest()
    if cache_key in fact_cache:
        print("‚úÖ Cache hit")
        return fact_cache[cache_key]

    loop = asyncio.get_event_loop()
    result_text = await loop.run_in_executor(executor, run_llm, data.claim)

    response = {"result": result_text}
    fact_cache[cache_key] = response
    return response
